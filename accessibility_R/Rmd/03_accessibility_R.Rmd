---
title: "Accessibility in R - testing efficient tools"
output: html_notebook
---

The right to the city inequalities analyses can benefit from metrics such as urban accessibility. In my background, I have already used this indicator to assess the location of developments, considering their different patterns and target audience. Most of my analyses also looked at public policies that could increase the vulnerable population's access to opportunities sprawled in the city.

Urban accessibility ($A_i$) can be defined as the number of opportunities ($W_j$) reachable from a specific location ($i$) given an spatial dependent function ($f(c_{ij})$). It can be calculated in several ways, and the most known are the cumulative and gravitational accessibility. The methods have a similar formulation (below), and the difference is on how the spatial data is considered.

$A_i = \sum_j{W_i*f(c_{ij})}$

**Cumulative accessibility** sums up the number of opportunities reachable from a certain point within a **threshold of travel time or distance**. Therefore all the opportunities more distant (in time or space) from the point $i$ are not considered in the indicator. 
Meanwhile, the **gravitational accessibility** does not depend on a threshold and uses a decay function. This function weights the opportunities regarding their proximity to the point of interest. The closer the opportunity, the higher its weight in the sum of the opportunity to calculate the indicator.

Most of the times I processed the **cumulative accessibility**, I used the `tidyverse` package, mostly `dplyr` and `magrittr`. However, I know it is not the most efficient approach. Therefore, I plan to develop some posts to showcase my learning process throughout this project. But in the end, I hope we will have a comparison among `base`, by using matrices, `tidyverse`, and `data.table`.

For this process, we can then consider two steps: 

1. Filtering the pairs with distance bigger than the threshold
2. Summing up the opportunities for each point to get the accessibility

```{r load files, echo = F}
library(tidyverse)
library(data.table)

zones_data <- read_csv('../data/zones_data.csv')
jobs <- zones_data[,c('Zona', 'Empregos')]
distances <- read_csv('../data/distance_matrix.csv')

threshold <- 10000
```

The threshold can be a random number, and I chose 10km for this analysis.

## Base - matrix

In a straight way, we can process the cumulative accessibility by using some simple matrix operations. To recap, the euclidean distances were processed by getting the geometric centers of each zone (`st_centroids` of `sf`) and the distances among these points (`st_distance`). The last function produced a matrix with all the distances among the origin-destination pairs. Therefore, the procedure was simple.

After evaluating the distances regarding the threshold and getting a `boolean matrix`, the next step was to associate the jobs (opportunities to be analyzed) to the destinations ($j$). The multiplication of the `boolean matrix` and the `array` (read as a matrix of one line and 517 columns) provides the cumulative accessibility for each origin ($i$) zone.

```{r matrix process}
acc_matrix <- function(distances, jobs, threshold){
  # converting the dfs to matrixes
  jobs_mt <- jobs$Empregos
  distances_mt <- as.matrix(distances)
  
  # filtering 
  dist_filt <- distances_mt <= threshold   # provides a boolean matrix
  df <- data.frame(origin = 1:nrow(distances),
                   acc = dist_filt %*% jobs_mt)
  return(df)
}
```

As mentioned before, the calculated distance data is in a matrix format. So the other two methods will already begin in a disadvantage due to the structure conversion. For comparison purposses, two functions for each method will be created, one to convert the `matrix` to a `data.frame` or `data.table` with the selected library and the other to process the accessibility.

## Tidyverse - dataframe



```{r tidyverse process}
# matrix convert
distance_df <- function(distances){
  df <- distances |>
    mutate(origin = 1:nrow(distances)) |>
    pivot_longer(cols = -origin,
                 names_to = 'destination',
                 values_to = 'distance') |>
    mutate(destination = as.numeric(str_remove(destination, 'V')))
  return(df)
}

# process accessibility
acc_tidy <- function(distances, jobs, threshold){
  df <- distances |>
    filter(distance <= threshold) |> 
    left_join(jobs, by = c('destination' = 'Zona')) |>
    group_by(origin) |>
    summarise(acc = sum(Empregos))
  return(df)
}
```

## Data.table

```{r data.table process}
# matrix convert
distance_dt <- function(distances){
  distances['origin'] <- 1:nrow(distances)
  dt <- melt(data = as.data.table(distances),
             id.vars = 'origin',
             variable.name = 'destination',
             value.name = 'distance')[, 
     `:=` (destination = as.numeric(str_remove(destination, 'V')))]
  return(dt)
}

acc_dt <- function(distances, jobs, threshold){
  # converting dfs to dt
  jobs_dt <- as.data.table(jobs, key = 'Zona')
  
  # filtering and updating destination
  dt <- distances[distance <= threshold, ][ # update
    jobs_dt, on = .(destination = Zona)][ # join
      , 
      .(acc = sum(Empregos)), # process
      by = origin] # group_by
  return(dt)
}

```

## Comparing the functions

```{r}
library(rbenchmark)

df_distances <- distance_df(distances)
dt_distances <- distance_dt(distances)

benchmark(base_matrix = acc_matrix(distances, jobs, threshold),
          df_whole = {
            df_distances <- distance_df(distances)
            acc_tidy(df_distances, jobs, threshold)
          },
          df_proc = acc_tidy(df_distances, jobs, threshold),
          dt_whole = {
            dt_distances <- distance_dt(distances)
            acc_dt(dt_distances, jobs, threshold)
          },
          dt_proc = acc_dt(dt_distances, jobs, threshold)
          )
```

