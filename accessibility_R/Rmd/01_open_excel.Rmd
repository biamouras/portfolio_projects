---
title: "Opening an excel file in R"
author: "Beatriz Moura dos Santos"
date: '2022-07-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First of all, we need to prepare the data. I will use the Origin-Destination survey from São Paulo’s Metrô, applied in 2017. The Metro enables the microdata and aggregate databases based on the traffic zones (regions) in the Metropolitan Region at its website. However, these aggregate databases are in excel files. Therefore this first post will focus on importing and cleaning the file.

## Loading the excel file

There are several packages in R to import excel files. I am used to use the `readxl`.

```{r load libraries}
library(readxl)

xls <- read_excel('data/Tab01_OD2017.xlsx')

head(xls)
```
You can see from the first few rows of the imported file that it has at least five rows without data. So let's read the file again without them.

```{r skipping rows}
xls <- read_excel('data/Tab01_OD2017.xlsx', skip = 5)
head(xls)
```

We are getting closer to get an ok header. However, see that there are some columns with the name in two rows. So I will get them and concatenate (`paste`).

```{r header names}
# first row with column names
names_1 <- colnames(xls)
names_1

# second row with column names
names_2 <- xls[1,]
names_2

# pasting each col name
new_names <- paste(names_1, names_2)
new_names
```

Note that now we have the full name of the column with some "noise" in it. But note that this noise has a pattern (`regex`) that we can remove using the function `str_remove` (remove the pattern) and `str_trim` (remove the whitespace) from the package `stringr`. 

```{r removing the noise}
regex <- '\\.\\.\\.\\d{1,2}'
new_names <- stringr::str_trim(stringr::str_remove(new_names, regex))
new_names
```

Now, we can add the `new_names` to the header of the Data Frame.

```{r applying new names}
colnames(xls) <- new_names
head(xls)
```

But see that the first row with the remaining of the column names and the previous structure (skipped one row before presenting the data) are still there. Also, the tail (bottom part) has some informataion that is not part of the database.

```{r tail of df}
tail(xls, 15)
```

Before removing these rows, one important thing is to correct the data types of each column. Now, all of them were read as `character`. 

```{r col types}
str(xls)
```

I know that most of the columns are not strings (only the second one is character type). So, I will apply a simple routine to change the other ones.

```{r change col type}
num_cols <- c(1, 3:11)
xls[num_cols] <- sapply(xls[num_cols], as.numeric)
str(xls)
```

Note that the rows with any information that were strings were converted to `NA`. So the remaining task is to remove all these empty rows.

```{r}
df <- xls[!is.na(xls[1]),]
head(df)
str(df)
```

Perfect! Now I have the organized data to continue the analysis.
```{r}
write.csv(df, 'data/zones_data.csv')
```


